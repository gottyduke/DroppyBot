import asyncio
import discord
import json
import os
import time

from discord.ext import commands
from shared import CogBase, cwd


class GPTHandler(CogBase, commands.Cog):
    def generate_help_info(self):
        help_info = [
            discord.Embed(
                color=discord.Color.blurple(),
                title="GPT",
                description="""
            ä½¿ç”¨chatGPTè¿›è¡Œæ–‡å­—ç”Ÿæˆ, powered by OpenAI

            å¯é™„åŠ æœ‰ä¸Šé™çš„ä¸Šä¸‹æ–‡, æ¨¡æ‹Ÿè¿žç»­å¯¹è¯(å‚è€ƒä½¿ç”¨æ–¹æ³•)

            å·²æä¾›`gpt-4-turbo`æˆ–`gpt-4o`**(æ–°!)**æ¨¡åž‹

            *2024/05/16 æ›´æ–°:*
            + ç§»é™¤äº†`gpt-3.5-turbo`, çŽ°æœ‰çš„`!gpt`å‘½ä»¤å°†ä¼šæŒ‡å‘`gpt-4-turbo`
            + æ·»åŠ äº†`gpt-4o`æ¨¡åž‹, çŽ°æœ‰çš„`!gpt4`å‘½ä»¤å°†ä¼šæŒ‡å‘å®ƒ

            2024/04/06 æ›´æ–°:*
            + è®­ç»ƒæ•°æ®å·²æ›´æ–°è‡³2023å¹´12æœˆä»½

            2024/03/24 æ›´æ–°:
            + gptæ¨¡åž‹`1106`->`0125`

            2023/12/18 æ›´æ–°:
            + å·²æ”¯æŒè”ç½‘æœç´¢, ç»“æžœåŸºäºŽäº’è”ç½‘æŸ¥æ‰¾

            2023/11/19 æ›´æ–°:
            + è®­ç»ƒæ•°æ®å·²æ›´æ–°è‡³2023å¹´4æœˆä»½
            + æ”¯æŒé•¿æ–‡æœ¬è¾“å…¥/è¾“å‡º(å‚è€ƒä½¿ç”¨æ–¹æ³•)
            + æ”¯æŒå›¾ç‰‡è¯†åˆ«/å›¾ç‰‡è¾…åŠ©çš„æ–‡å­—ç”Ÿæˆ(å‚è€ƒä½¿ç”¨æ–¹æ³•)
            """,
            ),
            discord.Embed(
                color=discord.Color.blurple(),
                title="å¦‚ä½•é€‰æ‹©ä¸åŒçš„æ¨¡åž‹?",
                description=f"""
            æœ¬æœºå™¨äººðŸ”§æŽ¥å…¥äº†OpenAI API, è‡ª3æœˆ27æ—¥ä¹ŸèŽ·å¾—äº†gpt-4æ¨¡åž‹APIçš„å†…æµ‹èµ„æ ¼, ~~å› æ­¤æä¾›gpt-3.5-turboå’Œgpt-4ä¸¤ç§æ¨¡åž‹~~

            ~~å…¶ä¸­`gpt-3.5-turbo`æ˜¯OpenAIé»˜è®¤chatGPTæ¨¡åž‹, è€Œ`gpt-4`åˆ™æ˜¯å…¶ä¹‹åŽçš„é«˜çº§ä»˜è´¹æ¨¡åž‹, è¯­è¨€èƒ½åŠ›å’Œé€»è¾‘æ€ç»´å¾—åˆ°äº†å¾ˆå¤§çš„åŠ å¼º~~

            ~~ä¸¤GPTæ¨¡åž‹çš„ä½¿ç”¨**æ²¡æœ‰**ç½‘é¡µç«¯çš„æ¯å°æ—¶æ¡æ•°é™åˆ¶. ä½†æ˜¯, ä½œä¸ºä¸€ä¸ªè®¡è´¹åˆ¶çš„API, åœ¨æ•´æ´»ç±»é—®é¢˜/é€»è¾‘éœ€æ±‚ä¸å¼ºçš„ä½¿ç”¨åœºæ™¯ä¸‹è¯·è°ƒç”¨`gpt-3.5-turbo`æ¨¡åž‹, æ­£å¸¸ä½¿ç”¨æ—¶åˆ™è°ƒç”¨`gpt-4`æ¨¡åž‹~~

            çŽ°å·²ç§»é™¤`gpt-3.5-turbo`, é»˜è®¤è®¾ç½®ä¸º`gpt-4-turbo`, é«˜çº§æ¨¡åž‹ä¸º`gpt-4o`, å°½æƒ…ä½¿ç”¨å§!

            """,
            ),
            discord.Embed(
                color=discord.Color.blurple(),
                title="å‘½ä»¤æŒ‡å—åŠå…¶ç¤ºä¾‹:",
                description=f"""
            ```
{self.compiled_gpt_cmd} : å‘chatGPTæ–‡å­—è¡¥å…¨æä¾›prompt
{self.compiled_gpt4_cmd} : å‘chatGPTæ–‡å­—è¡¥å…¨æä¾›prompt, æŒ‡å®šä½¿ç”¨gpt-4æ¨¡åž‹
{self.compiled_gptinit_cmd} : è®¾å®šä¸ªäººä¸“å±žçš„chatGPTè®¾å®šè¯­å¥, ä¾‹å¦‚äººæ ¼æ¨¡æ‹Ÿ/é£Žæ ¼è®¾å®š```

            å–è‡ªäºŽç¾¤å‹æ—¥å¸¸ä½¿ç”¨åœºæ™¯ðŸ˜…

            è¯¢é—®`gpt-3.5-turbo(é»˜è®¤chatGPTæ¨¡åž‹)`å¯¹äºŽæ”€ç™»é«˜å±±çš„çœ‹æ³•:
            ```
{self.compiled_gpt_cmd} ä½ å¯¹æ–¼çˆ¬é«˜å±±æœ‰ä»€ä¹ˆçœ‹æ³•?```
            ä½¿ç”¨`gpt-4`ç”¨è¯—çš„å½¢å¼è¯æ˜Žç´ æ•°çš„æ— ç©·æ€§:
            ```
{self.compiled_gpt4_cmd} ç”¨è¯—çš„å½¢å¼è¯æ˜Žç´ æ•°çš„æ— ç©·æ€§```
            è¾…åŠ©ç”Ÿäº§åŠ›:
            ```
{self.compiled_gpt4_cmd} å¦‚ä½•å†™ä¸€ä¸ªå¥½çš„å¼€é¢˜æŠ¥å‘Š? æˆ‘çš„æ–¹å‘æ˜¯XXXX, éœ€è¦æ³¨é‡XXXX, XXXX```
            è¿›è¡ŒåŒè¯­å¯¹ç…§ç¿»è¯‘:
            ```
{self.compiled_gpt4_cmd} "å‘½é‡Œæœ‰æ—¶ç»ˆé¡»æœ‰ï¼Œå‘½é‡Œæ— æ—¶èŽ«å¼ºæ±‚"è¿™å¥è¯ç”¨ä¿„è¯­å¦‚ä½•ç¿»è¯‘? è¦æ±‚å°½é‡ä¿¡è¾¾é›…, å¹¶é€å¥é™„åŠ ä¸­ä¿„åŒè¯­å¯¹ç…§å’Œè§£é‡Š```
            """,
            ),
            discord.Embed(
                color=discord.Color.blurple(),
                title="é™„åŠ ä¸Šä¸‹æ–‡è¿›è¡Œè¿žç»­å¯¹è¯",
                description=f"""
            æ¯æ¬¡ä½¿ç”¨`!gpt`/`!gpt4`å‘½ä»¤æ—¶, è¿™ä¾¿å¼€å¯äº†ä¸€ä¸ª**æ–°çš„å¯¹è¯**.

            è‹¥è¦è·Ÿéšæ­¤å¯¹è¯ä¸Šä¸‹æ–‡è¿›è¡Œè¿žç»­å¯¹è¯, è¯·å³é”®è¯¥ðŸ¤–çš„å›žç­”, é€‰æ‹©å›žå¤å³å¯.

            å½“è·Ÿéšä¸Šä¸‹æ–‡ç»§ç»­å¯¹è¯æ—¶, **ä¸éœ€è¦å†æ¬¡ä½¿ç”¨**`!gpt`/`!gpt4`å‘½ä»¤, è¯·ç›´æŽ¥è¾“å…¥ä½ çš„æ–‡å­—.
            """,
            ).set_image(url="https://i.postimg.cc/j5SxZLrL/reply.png"),
            discord.Embed(
                color=discord.Color.blurple(),
                title="å…³äºŽGPTè®¾å®šè¯­å¥",
                description=f"""
            GPTè®¾å®šè¯­å¥æ˜¯æ¯æ¬¡å¯¹è¯æ—¶å„ç”¨æˆ·ä¸“å±žçš„å‰ç½®è®¾å®š, åˆç§°ä¸ºè°ƒæ•™, å’’è¯­ç­‰

            ä¾‹å¦‚, ä½¿GPTæ‰®æ¼”ä¸€ä¸ªçŒ«å¨˜:
            ```
{self.compiled_gptinit_cmd} ä½ å°†æ‰®æ¼”ä¸€ä¸ªçŒ«å¨˜, ç§°å‘¼æˆ‘ä¸ºä¸»äºº, å¼€å¤´å’Œç»“å°¾éƒ½è¦ä½¿ç”¨å–µè¯­æ°”å’Œå¯çˆ±é£Žæ ¼```

            è‹¥è¦æ¸…ç©ºGPTè®¾å®šè¯­å¥, åˆ™ä½¿ç”¨ä¸åŠ æ–‡å­—çš„`{self.compiled_gptinit_cmd}`å‘½ä»¤
            """,
            ),
            discord.Embed(
                color=discord.Color.blurple(),
                title="é™„åŠ æ–‡ä»¶ç”¨äºŽæ–‡å­—ç”Ÿæˆ",
                description=f"""
            ä½¿ç”¨GPTå‘½ä»¤æ—¶, å°†æ–‡ä»¶æ‹–æ‹½è‡³è¾“å…¥æ å³å¯é™„åŠ æ­¤æ–‡ä»¶ç”¨äºŽä¸‹æ¬¡æ–‡å­—ç”Ÿæˆ

            æœºå™¨äººæœªæ”¯æŒå¦‚`.docx`, `.pdf`ç­‰æ ¼å¼çš„è‡ªåŠ¨è½¬æ¢, æ–‡æ¡£ç±»æ–‡ä»¶è¯·è‡ªè¡Œè½¬æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼(`.txt`), ä»£ç ç±»æ–‡ä»¶å¯ç›´æŽ¥é™„åŠ 

            å¯ä¸€æ¬¡é™„åŠ å¤šä¸ªæ–‡ä»¶
            """,
            ).set_image(url="https://i.postimg.cc/cLqK5yXp/1.png"),
            discord.Embed(
                color=discord.Color.blurple(),
                title="é™„åŠ å›¾ç‰‡è¿›è¡Œå›¾ç‰‡è¯†åˆ«/è¾…åŠ©ç”Ÿæˆ",
                description=f"""
            ä½¿ç”¨GPTå‘½ä»¤æ—¶, å°†å›¾ç‰‡æ‹–æ‹½/å¤åˆ¶è‡³è¾“å…¥æ å³å¯é™„åŠ æ­¤æ–‡ä»¶ç”¨äºŽä¸‹æ¬¡å›¾ç‰‡è¯†åˆ«/è¾…åŠ©ç”Ÿæˆ

            å›¾ç‰‡ç±»æ–‡ä»¶ä¸æ”¯æŒå›¾å½¢äº¤æ¢æ ¼å¼(`.gif`, å³åŠ¨å›¾), ä»…æ”¯æŒå¸¸è§å›¾ç‰‡æ ¼å¼å¦‚`.jpg`, `.png`

            å¯ä¸€æ¬¡é™„åŠ å¤šä¸ªæ–‡ä»¶
            """,
            ).set_image(url="https://i.postimg.cc/wxPqbL9t/2.png"),
        ]

        return help_info

    def __init__(self):
        self.internal_latency = None
        self.active_model = self.config.gpt.model.default
        self.max_token = next(
            model
            for model in self.config.gpt.model.spec
            if model["name"] == self.active_model
        )["max_token"]
        self.user_init: dict[int, str] = self.load_user_init()
        self.user_ctx: dict[int, list[tuple[discord.Message, discord.Message]]] = {}
        self.user_token = int(
            self.max_token * self.config.gpt.contextual.max_ctx_percentage
        )

        self.private_query = False

        self.compiled_gpt_cmd = f"{self.bot.command_prefix}gpt"
        self.compiled_gpt4_cmd = f"{self.bot.command_prefix}gpt4"
        self.compiled_gptinit_cmd = f"{self.bot.command_prefix}gptinit"

        self.help_info["GPT"] = self.generate_help_info()

    def load_user_init(self):
        user_init = {}
        user_init_path = os.path.join(cwd, self.config.gpt.user_init_path)
        if os.path.exists(user_init_path):
            try:
                with open(user_init_path, "rb") as f:
                    for user, init in json.load(f).items():
                        user_init[int(user)] = str(init)
            except Exception:
                user_init = {}

        return user_init

    def save_user_init(self):
        for user, init in self.user_init.copy().items():
            if init is None or init == "" or init.isspace():
                del self.user_init[user]

        user_init_path = os.path.join(cwd, self.config.gpt.user_init_path)
        with open(user_init_path, "w", encoding="utf-8") as f:
            json.dump(self.user_init, f, indent=4, ensure_ascii=False)

    async def retrieve_conversation(self, msg: discord.Message):
        """
        retrieve ongoing conversation as contextual input
        conversation should be in request-and-reply style
        """

        prompts = []
        valid_gpt = False
        # check if user replied to a gpt response for a "contextual conversation"
        ref = msg.reference
        if ref is None or ref.resolved is None:
            return None

        answer = ref.resolved
        if answer.author != self.bot.user:
            return None

        retrieval = self.config.gpt.contextual.max_ctx_per_user
        while retrieval > 0:
            if answer is None:
                break
            # gpt response is an embed
            prompts.append(
                {"role": "assistant", "content": answer.embeds[0].description}
            )

            # using message id to resolve manually, due to discord API not attempting to chain de-reference
            question = answer.reference
            if question is None or question.message_id is None:
                break

            question = await msg.channel.fetch_message(question.message_id)
            if question is None or question.author != msg.author:
                break

            prompt = question.content
            prompts.append({"role": "user", "content": prompt})

            if self.compiled_gpt_cmd in prompt:
                valid_gpt = True

            if question.reference is None or question.reference.message_id is None:
                break
            answer = await msg.channel.fetch_message(question.reference.message_id)

            retrieval -= 1
        return prompts if valid_gpt else None

    async def request_and_reply(
        self, prompt, requests, msg: discord.Message, reply: discord.Message
    ):
        """
        make actual chatGPT request with openAI endpoint
        update user context storage and log the action
        """

        # telemetry
        tele_prompt_token = 0
        tele_res_token = 0

        # request for chat completion
        completion = await asyncio.to_thread(
            self.endpoint.chat.completions.create,
            model=self.active_model,
            messages=requests,
            tools=self.config.gpt.tools,
        )

        tele_prompt_token += completion.usage.prompt_tokens - len(prompt)
        tele_res_token += completion.usage.completion_tokens

        # if tools are called
        if (
            completion.choices[0].message.tool_calls is not None
            and len(completion.choices[0].message.tool_calls) != 0
        ):
            tools = self.bot.get_cog("GPTTools")
            if tools is not None:
                tool_response = tools.process_tool_call(
                    completion.choices[0].message.tool_calls[0]
                )

                requests.append(
                    {
                        "role": "assistant",
                        "tool_calls": completion.choices[0].message.tool_calls,
                    }
                )
                requests.append(
                    {
                        "role": "tool",
                        "content": str(tool_response),
                        "tool_call_id": completion.choices[0].message.tool_calls[0].id,
                    }
                )

            # if tool call failed, fall back to default completion, otherwise commit
            completion = await asyncio.to_thread(
                self.endpoint.chat.completions.create,
                model=self.active_model,
                messages=requests,
            )

            tele_prompt_token += completion.usage.prompt_tokens - len(prompt)
            tele_res_token += completion.usage.completion_tokens

        # get perf latency
        latency = time.perf_counter() - self.internal_latency
        latency = int(round(latency * 1000))

        # respond to user
        response = self.format_response(completion.choices[0].message.content)
        await reply.edit(embed=self.as_embed(response[0]))
        for res in response[1:]:
            await reply.reply(embed=self.as_embed(res))

        # save user specific context
        aid = msg.author.id
        if aid in self.user_ctx and self.user_ctx[aid] is not None:
            if self.user_init[aid].strip() != "":
                self.user_ctx[aid].append((msg, reply))

        if not self.private_query:
            context = f"({completion.usage.prompt_tokens - len(prompt)})+{len(prompt)}"
            completion_token = completion.usage.completion_tokens
            tally = completion.usage.total_tokens
            telemetry = f"[{context}+{completion_token}={tally}]({reply.jump_url})"
            self.log(
                msg,
                f"{self.active_model} {telemetry} {latency}ms\n```{prompt}```",
            )

    @commands.hybrid_command()
    @CogBase.failsafe(CogBase.config.gpt.thinking_indicator)
    async def gpt(self, ctx: commands.Context, *, prompt):
        """
        gpt-4-turbo, 128k
        """

        ref = await self.get_ctx_ref(ctx)

        self.internal_latency = time.perf_counter()

        # placeholder
        embed = self.as_embed(self.config.gpt.thinking_indicator, ctx.author)
        await ref.edit(embed=embed)

        # calc max tokens allowed for contextual gpt
        aid = ctx.author.id
        prompts = []
        tokens = self.user_token
        tokens -= len(prompt)

        # prepend system init, for RP purpose or preset guidelines
        if aid in self.user_init and self.user_init[aid] is not None:
            prompts.append({"role": "system", "content": self.user_init[aid]})
            tokens -= len(self.user_init[aid])

        # construct context
        if aid in self.user_ctx and self.user_ctx[aid] is not None:
            for history, answer in reversed(self.user_ctx[aid]):
                if (
                    ctx.message.created_at - history.created_at
                ).total_seconds() <= self.config.gpt.contextual.in_memory_timeframe:
                    consumed_tokens = len(history.content) + len(
                        answer.embeds[0].description
                    )
                    tokens -= consumed_tokens
                    if tokens >= 0:
                        prompts.append({"role": "user", "content": history.content})
                        prompts.append(
                            {
                                "role": "assistant",
                                "content": answer.embeds[0].description,
                            }
                        )
                    else:
                        break
                else:
                    self.user_ctx[aid].remove((history, answer))

        # append file inputs
        if len(ctx.message.attachments) >= 1:
            for file in ctx.message.attachments:
                if file.content_type.startswith("image"):
                    prompts.append(
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": file.proxy_url,
                                "detail": self.config.gpt.model.vision_fidelity,
                            },
                        }
                    )
                if file.content_type.startswith("text"):
                    prompts.append(
                        {
                            "role": "user",
                            "content": f"{file.filename}:\n```{(await file.read()).decode('utf-8')}```",
                        }
                    )
        prompts.append({"role": "user", "content": prompt})

        # request for chat completion
        await self.request_and_reply(prompt, prompts, ctx.message, ref)
        self.active_model = self.config.gpt.model.default

    @commands.hybrid_command()
    async def gpt4(self, ctx: commands.Context, *, prompt):
        """
        gpt-4o, 128k
        """

        spec = str(self.active_model)
        self.active_model = self.config.gpt.model.advanced
        await self.gpt(ctx, prompt=prompt)
        self.active_model = spec

    @commands.command()
    async def gptp(self, ctx: commands.Context, *, prompt):
        self.private_query = True
        await self.gpt4(ctx, prompt=prompt)
        self.private_query = False

    @commands.hybrid_command()
    @CogBase.failsafe()
    async def gptinit(self, ctx: commands.Context, *, init=None):
        """
        Set startup prompt for your own
        """

        ref = await self.get_ctx_ref(ctx)

        aid = ctx.author.id
        original = ""
        if aid in self.user_init and self.user_init[aid] is not None:
            original = f"\n```{self.user_init[aid]}```->"
        self.user_init[aid] = init

        await ref.edit(
            embed=self.as_embed(
                f"æ‚¨çš„GPTè®¾å®šå·²æ›´æ”¹!{original}\n```{init}```", ctx.author
            )
        )
        self.log(
            ctx.message,
            f"gpt-init ({len(init) if init is not None else 0}) {original}\n```{init}```",
        )
        self.save_user_init()

    async def build_contextual(self, ctx: discord.Message):
        # check if the replied message is also a replied message a.k.a a gpt response
        prompts = []
        prompt = ctx.content
        prompts.append({"role": "user", "content": prompt})

        conversation = await self.retrieve_conversation(ctx)
        if conversation is None:
            return

        # replies from other commands
        if len(conversation) <= 1:
            return

        prompts += conversation

        # placeholder
        embed = self.as_embed(self.config.gpt.thinking_indicator, ctx.author)
        reply = await ctx.reply(embed=embed)

        # prepend system init, for RP purpose or preset guidelines
        aid = ctx.author.id
        if aid in self.user_init and self.user_init[aid] is not None:
            prompts.append({"role": "system", "content": self.user_init[aid]})

        # request for chat completion
        prompts.reverse()
        spec = str(self.active_model)
        self.active_model = self.config.gpt.model.advanced
        await self.request_and_reply(prompt, prompts, ctx, reply)
        self.active_model = spec

    # listen for inputs
    @commands.Cog.listener()
    async def on_message(self, ctx: discord.Message):
        if await self.prepass(ctx) is None:
            return

        ref = ctx.reference
        if ref is None or ref.resolved is None:
            return

        # check if user replied to a gpt response for a "contextual conversation"
        if ref.resolved.author != self.bot.user:
            return

        self.internal_latency = time.perf_counter()
        await self.build_contextual(ctx)
